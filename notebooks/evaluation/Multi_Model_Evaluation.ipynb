{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-Model Evaluation & Comparison\n",
    "\n",
    "Compare green space detection models:\n",
    "- **Random Forest (RF)**\n",
    "- **Support Vector Machine (SVM)**\n",
    "- **U-Net Deep Learning**\n",
    "\n",
    "Against ground truth:\n",
    "- **WorldCover 2021** (ESA satellite-derived land cover)\n",
    "- **OSM** (OpenStreetMap crowd-sourced green spaces)\n",
    "- **NDVI Filter** (simple vegetation index threshold)\n",
    "\n",
    "Metrics: Accuracy, Precision, Recall, F1-Score, IoU, Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, jaccard_score\n",
    ")\n",
    "from scipy.ndimage import median_filter\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow for U-Net (optional)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    HAS_TENSORFLOW = True\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    HAS_TENSORFLOW = False\n",
    "    print(\"TensorFlow not available - U-Net evaluation will be skipped\")\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Test city (excluded from training)\nCITY = \"London\"\n\n# Base paths - using relative paths from project root\n# Run notebooks from the project root directory\nimport os\n\n# Find project root (go up from notebooks/evaluation/)\nif os.path.exists(\"data\") and os.path.exists(\"models\"):\n    PROJECT_ROOT = os.getcwd()\nelif os.path.exists(\"../../data\") and os.path.exists(\"../../models\"):\n    PROJECT_ROOT = os.path.abspath(\"../..\")\nelse:\n    PROJECT_ROOT = os.getcwd()\n    print(f\"Warning: Could not detect project root. Using: {PROJECT_ROOT}\")\n\n# Derived paths\nDATA_PATH = os.path.join(PROJECT_ROOT, \"data\")\nMODELS_PATH = os.path.join(PROJECT_ROOT, \"models\")\n\n# Data paths\nSTACK_PATH = os.path.join(DATA_PATH, \"sentinel_stacks\", f\"{CITY}_MultiMonth_stack.tif\")\nWORLDCOVER_PATH = os.path.join(DATA_PATH, \"worldcover\", f\"{CITY}_WorldCover_2021.tif\")\nOSM_PATH = os.path.join(DATA_PATH, \"sentinel_raw\", CITY, f\"{CITY}_OSM_labels.tif\")\nAOI_PATH = os.path.join(DATA_PATH, \"aois\", f\"{CITY}.geojson\")\n\n# Model paths\nRF_MODEL_PATH = os.path.join(MODELS_PATH, \"random_forest_model.pkl\")\nRF_SCALER_PATH = os.path.join(MODELS_PATH, \"feature_scaler.pkl\")\nSVM_MODEL_PATH = os.path.join(MODELS_PATH, \"svm_model.pkl\")\nSVM_SCALER_PATH = os.path.join(MODELS_PATH, \"svm_scaler.pkl\")\nUNET_MODEL_PATH = os.path.join(MODELS_PATH, \"unet_model.keras\")\nUNET_NORM_PATH = os.path.join(MODELS_PATH, \"unet_normalization_params.json\")\n\n# Output folder\nOUTPUT_FOLDER = os.path.join(PROJECT_ROOT, \"outputs\", \"multi_model_evaluation\", CITY)\nos.makedirs(OUTPUT_FOLDER, exist_ok=True)\n\n# Prediction settings\nRF_THRESHOLD = 0.55\nSVM_THRESHOLD = 0.5\nUNET_THRESHOLD = 0.5\nNDVI_THRESHOLD = 0.3\nAPPLY_SMOOTHING = True\nUNET_PATCH_SIZE = 64\nUNET_OVERLAP = 16\n\n# WorldCover green classes\nGREEN_CLASSES = [10, 20, 30]  # Tree, Shrubland, Grassland\n\nprint(f\"Evaluating: {CITY}\")\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Data path: {DATA_PATH}\")\nprint(f\"Models path: {MODELS_PATH}\")\nprint(f\"Output folder: {OUTPUT_FOLDER}\")\nprint(f\"\\nModel availability:\")\nprint(f\"  RF:    {'Available' if os.path.exists(RF_MODEL_PATH) else 'Not found'}\")\nprint(f\"  SVM:   {'Available' if os.path.exists(SVM_MODEL_PATH) else 'Not found'}\")\nprint(f\"  U-Net: {'Available' if os.path.exists(UNET_MODEL_PATH) and HAS_TENSORFLOW else 'Not available'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentinel-2 stack\n",
    "print(f\"Loading Sentinel-2 stack for {CITY}...\")\n",
    "with rasterio.open(STACK_PATH) as src:\n",
    "    X_stack = src.read()\n",
    "    profile = src.profile.copy()\n",
    "    transform = src.transform\n",
    "    crs = src.crs\n",
    "\n",
    "n_bands, height, width = X_stack.shape\n",
    "print(f\"  Shape: {X_stack.shape} ({n_bands} bands, {height}x{width} pixels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WorldCover and reproject\n",
    "print(f\"\\nLoading WorldCover...\")\n",
    "with rasterio.open(WORLDCOVER_PATH) as src:\n",
    "    wc_data = src.read(1)\n",
    "    wc_transform = src.transform\n",
    "    wc_crs = src.crs\n",
    "\n",
    "worldcover = np.empty((height, width), dtype=np.uint8)\n",
    "reproject(\n",
    "    source=wc_data,\n",
    "    destination=worldcover,\n",
    "    src_transform=wc_transform,\n",
    "    src_crs=wc_crs,\n",
    "    dst_transform=transform,\n",
    "    dst_crs=crs,\n",
    "    resampling=Resampling.nearest\n",
    ")\n",
    "\n",
    "worldcover_green = np.isin(worldcover, GREEN_CLASSES).astype(np.uint8)\n",
    "wc_pct = 100 * worldcover_green.sum() / worldcover_green.size\n",
    "print(f\"  WorldCover green: {wc_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OSM labels\n",
    "print(f\"\\nLoading OSM labels...\")\n",
    "if os.path.exists(OSM_PATH):\n",
    "    with rasterio.open(OSM_PATH) as src:\n",
    "        osm_data = src.read(1)\n",
    "        osm_transform_src = src.transform\n",
    "        osm_crs_src = src.crs\n",
    "    \n",
    "    if osm_data.shape != (height, width):\n",
    "        osm_labels = np.empty((height, width), dtype=np.uint8)\n",
    "        reproject(\n",
    "            source=osm_data,\n",
    "            destination=osm_labels,\n",
    "            src_transform=osm_transform_src,\n",
    "            src_crs=osm_crs_src,\n",
    "            dst_transform=transform,\n",
    "            dst_crs=crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "    else:\n",
    "        osm_labels = osm_data\n",
    "    \n",
    "    osm_pct = 100 * osm_labels.sum() / osm_labels.size\n",
    "    print(f\"  OSM green: {osm_pct:.2f}%\")\n",
    "else:\n",
    "    print(\"  OSM labels not found!\")\n",
    "    osm_labels = None\n",
    "    osm_pct = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# Load Random Forest\n",
    "if os.path.exists(RF_MODEL_PATH) and os.path.exists(RF_SCALER_PATH):\n",
    "    print(\"Loading Random Forest model...\")\n",
    "    rf_model = joblib.load(RF_MODEL_PATH)\n",
    "    rf_scaler = joblib.load(RF_SCALER_PATH)\n",
    "    models['RF'] = {'model': rf_model, 'scaler': rf_scaler}\n",
    "    print(f\"  RF loaded: {type(rf_model).__name__}\")\n",
    "else:\n",
    "    print(\"  RF model not found\")\n",
    "\n",
    "# Load SVM\n",
    "if os.path.exists(SVM_MODEL_PATH) and os.path.exists(SVM_SCALER_PATH):\n",
    "    print(\"Loading SVM model...\")\n",
    "    svm_model = joblib.load(SVM_MODEL_PATH)\n",
    "    svm_scaler = joblib.load(SVM_SCALER_PATH)\n",
    "    models['SVM'] = {'model': svm_model, 'scaler': svm_scaler}\n",
    "    print(f\"  SVM loaded: {type(svm_model).__name__}\")\n",
    "else:\n",
    "    print(\"  SVM model not found\")\n",
    "\n",
    "# Load U-Net\n",
    "if HAS_TENSORFLOW and os.path.exists(UNET_MODEL_PATH):\n",
    "    print(\"Loading U-Net model...\")\n",
    "    \n",
    "    # Custom objects for U-Net\n",
    "    def dice_coef(y_true, y_pred, smooth=1):\n",
    "        y_true_f = tf.keras.backend.flatten(y_true)\n",
    "        y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "        return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "    def dice_loss(y_true, y_pred):\n",
    "        return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "    def combined_loss(y_true, y_pred):\n",
    "        return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    \n",
    "    unet_model = keras.models.load_model(\n",
    "        UNET_MODEL_PATH,\n",
    "        custom_objects={'dice_coef': dice_coef, 'combined_loss': combined_loss}\n",
    "    )\n",
    "    models['U-Net'] = {'model': unet_model}\n",
    "    print(f\"  U-Net loaded\")\n",
    "else:\n",
    "    print(\"  U-Net model not available\")\n",
    "\n",
    "print(f\"\\nModels loaded: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "green_percentages = {'WorldCover': wc_pct}\n",
    "if osm_pct is not None:\n",
    "    green_percentages['OSM'] = osm_pct\n",
    "\n",
    "# Prepare data for pixel-based models\n",
    "X_flat = X_stack.reshape(n_bands, -1).T\n",
    "valid_mask = ~np.isnan(X_flat).any(axis=1)\n",
    "X_valid = X_flat[valid_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest prediction\n",
    "if 'RF' in models:\n",
    "    print(\"Generating RF prediction...\")\n",
    "    rf_model = models['RF']['model']\n",
    "    rf_scaler = models['RF']['scaler']\n",
    "    \n",
    "    X_scaled = rf_scaler.transform(X_valid)\n",
    "    \n",
    "    # Use predict_proba if available\n",
    "    if hasattr(rf_model, 'predict_proba'):\n",
    "        y_proba = rf_model.predict_proba(X_scaled)\n",
    "        y_pred = (y_proba[:, 1] >= RF_THRESHOLD).astype(int)\n",
    "    else:\n",
    "        y_pred = rf_model.predict(X_scaled)\n",
    "    \n",
    "    # Reconstruct full map\n",
    "    rf_prediction = np.full(height * width, np.nan)\n",
    "    rf_prediction[valid_mask] = y_pred\n",
    "    rf_prediction = rf_prediction.reshape(height, width)\n",
    "    \n",
    "    # Apply smoothing\n",
    "    if APPLY_SMOOTHING:\n",
    "        nan_mask = np.isnan(rf_prediction)\n",
    "        temp_map = np.where(nan_mask, 0, rf_prediction).astype(np.float32)\n",
    "        temp_filtered = median_filter(temp_map, size=3)\n",
    "        rf_prediction = np.where(nan_mask, np.nan, np.round(temp_filtered))\n",
    "    \n",
    "    predictions['RF'] = rf_prediction\n",
    "    rf_pct = 100 * np.nansum(rf_prediction == 1) / np.sum(~np.isnan(rf_prediction))\n",
    "    green_percentages['RF'] = rf_pct\n",
    "    print(f\"  RF green: {rf_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM prediction\n",
    "if 'SVM' in models:\n",
    "    print(\"Generating SVM prediction...\")\n",
    "    svm_model = models['SVM']['model']\n",
    "    svm_scaler = models['SVM']['scaler']\n",
    "    \n",
    "    X_scaled = svm_scaler.transform(X_valid)\n",
    "    \n",
    "    # Use predict_proba if available (for SVC with probability=True)\n",
    "    if hasattr(svm_model, 'predict_proba'):\n",
    "        y_proba = svm_model.predict_proba(X_scaled)\n",
    "        y_pred = (y_proba[:, 1] >= SVM_THRESHOLD).astype(int)\n",
    "    else:\n",
    "        y_pred = svm_model.predict(X_scaled)\n",
    "    \n",
    "    # Reconstruct full map\n",
    "    svm_prediction = np.full(height * width, np.nan)\n",
    "    svm_prediction[valid_mask] = y_pred\n",
    "    svm_prediction = svm_prediction.reshape(height, width)\n",
    "    \n",
    "    # Apply smoothing\n",
    "    if APPLY_SMOOTHING:\n",
    "        nan_mask = np.isnan(svm_prediction)\n",
    "        temp_map = np.where(nan_mask, 0, svm_prediction).astype(np.float32)\n",
    "        temp_filtered = median_filter(temp_map, size=3)\n",
    "        svm_prediction = np.where(nan_mask, np.nan, np.round(temp_filtered))\n",
    "    \n",
    "    predictions['SVM'] = svm_prediction\n",
    "    svm_pct = 100 * np.nansum(svm_prediction == 1) / np.sum(~np.isnan(svm_prediction))\n",
    "    green_percentages['SVM'] = svm_pct\n",
    "    print(f\"  SVM green: {svm_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net prediction\n",
    "if 'U-Net' in models:\n",
    "    print(\"Generating U-Net prediction...\")\n",
    "    unet_model = models['U-Net']['model']\n",
    "    \n",
    "    # Normalize image function\n",
    "    def normalize_image(image):\n",
    "        image = image.astype(np.float32)\n",
    "        for c in range(image.shape[-1]):\n",
    "            channel = image[:, :, c]\n",
    "            min_val = np.nanmin(channel)\n",
    "            max_val = np.nanmax(channel)\n",
    "            if max_val > min_val:\n",
    "                image[:, :, c] = (channel - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                image[:, :, c] = 0\n",
    "        return image\n",
    "    \n",
    "    # Predict full image with sliding window\n",
    "    def predict_full_image(model, image, patch_size=64, overlap=16):\n",
    "        H, W, C = image.shape\n",
    "        stride = patch_size - overlap\n",
    "        \n",
    "        # Pad image\n",
    "        pad_h = (patch_size - H % stride) % stride\n",
    "        pad_w = (patch_size - W % stride) % stride\n",
    "        image_padded = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "        \n",
    "        H_pad, W_pad, _ = image_padded.shape\n",
    "        \n",
    "        prediction_sum = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "        count = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "        \n",
    "        image_norm = normalize_image(image_padded)\n",
    "        \n",
    "        for i in range(0, H_pad - patch_size + 1, stride):\n",
    "            for j in range(0, W_pad - patch_size + 1, stride):\n",
    "                patch = image_norm[i:i+patch_size, j:j+patch_size, :]\n",
    "                patch_batch = np.expand_dims(patch, axis=0)\n",
    "                \n",
    "                pred = model.predict(patch_batch, verbose=0)[0, :, :, 0]\n",
    "                \n",
    "                prediction_sum[i:i+patch_size, j:j+patch_size] += pred\n",
    "                count[i:i+patch_size, j:j+patch_size] += 1\n",
    "        \n",
    "        prediction = prediction_sum / np.maximum(count, 1)\n",
    "        prediction = prediction[:H, :W]\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    # Prepare image (H, W, C)\n",
    "    X_image = X_stack.transpose(1, 2, 0)\n",
    "    \n",
    "    # Get probability map\n",
    "    unet_proba = predict_full_image(\n",
    "        unet_model, X_image,\n",
    "        patch_size=UNET_PATCH_SIZE,\n",
    "        overlap=UNET_OVERLAP\n",
    "    )\n",
    "    \n",
    "    # Threshold to binary\n",
    "    unet_prediction = (unet_proba >= UNET_THRESHOLD).astype(np.float32)\n",
    "    \n",
    "    # Apply smoothing\n",
    "    if APPLY_SMOOTHING:\n",
    "        unet_prediction = median_filter(unet_prediction, size=3)\n",
    "        unet_prediction = np.round(unet_prediction)\n",
    "    \n",
    "    predictions['U-Net'] = unet_prediction\n",
    "    unet_pct = 100 * unet_prediction.sum() / unet_prediction.size\n",
    "    green_percentages['U-Net'] = unet_pct\n",
    "    print(f\"  U-Net green: {unet_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDVI Green Filter\n",
    "print(\"\\nCalculating NDVI Green Filter...\")\n",
    "\n",
    "def calculate_ndvi(nir, red):\n",
    "    nir = nir.astype(float)\n",
    "    red = red.astype(float)\n",
    "    return (nir - red) / (nir + red + 1e-8)\n",
    "\n",
    "# Band indices (21-band stack)\n",
    "ndvi_april = calculate_ndvi(X_stack[3], X_stack[2])\n",
    "ndvi_august = calculate_ndvi(X_stack[10], X_stack[9])\n",
    "ndvi_november = calculate_ndvi(X_stack[17], X_stack[16])\n",
    "\n",
    "ndvi_mean = np.nanmean(np.stack([ndvi_april, ndvi_august, ndvi_november]), axis=0)\n",
    "green_filter = (ndvi_mean > NDVI_THRESHOLD).astype(np.uint8)\n",
    "\n",
    "predictions['NDVI Filter'] = green_filter\n",
    "gf_pct = 100 * green_filter.sum() / green_filter.size\n",
    "green_percentages['NDVI Filter'] = gf_pct\n",
    "print(f\"  NDVI Filter (>{NDVI_THRESHOLD}): {gf_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, name):\n",
    "    \"\"\"Calculate classification metrics.\"\"\"\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    \n",
    "    valid = ~(np.isnan(y_true_flat) | np.isnan(y_pred_flat))\n",
    "    y_true_clean = y_true_flat[valid].astype(int)\n",
    "    y_pred_clean = y_pred_flat[valid].astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'Method': name,\n",
    "        'Accuracy': accuracy_score(y_true_clean, y_pred_clean),\n",
    "        'Precision': precision_score(y_true_clean, y_pred_clean, zero_division=0),\n",
    "        'Recall': recall_score(y_true_clean, y_pred_clean, zero_division=0),\n",
    "        'F1-Score': f1_score(y_true_clean, y_pred_clean, zero_division=0),\n",
    "        'IoU': jaccard_score(y_true_clean, y_pred_clean, zero_division=0),\n",
    "        'Pred Green %': 100 * y_pred_clean.sum() / len(y_pred_clean),\n",
    "        'GT Green %': 100 * y_true_clean.sum() / len(y_true_clean),\n",
    "        'Diff %': 100 * (y_pred_clean.sum() - y_true_clean.sum()) / len(y_true_clean)\n",
    "    }\n",
    "    \n",
    "    return metrics, confusion_matrix(y_true_clean, y_pred_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "results = []\n",
    "confusion_matrices = {}\n",
    "\n",
    "# Each model vs WorldCover\n",
    "for model_name, pred in predictions.items():\n",
    "    metrics, cm = calculate_metrics(worldcover_green, pred, f'{model_name} vs WorldCover')\n",
    "    results.append(metrics)\n",
    "    confusion_matrices[f'{model_name} vs WorldCover'] = cm\n",
    "\n",
    "# Each model vs OSM (if available)\n",
    "if osm_labels is not None:\n",
    "    for model_name, pred in predictions.items():\n",
    "        metrics, cm = calculate_metrics(osm_labels, pred, f'{model_name} vs OSM')\n",
    "        results.append(metrics)\n",
    "        confusion_matrices[f'{model_name} vs OSM'] = cm\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "df_results.to_csv(os.path.join(OUTPUT_FOLDER, 'metrics_comparison.csv'), index=False)\n",
    "print(f\"\\nMetrics saved to: {OUTPUT_FOLDER}/metrics_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare RGB\n",
    "rgb = X_stack[[9, 8, 7], :, :].transpose(1, 2, 0)\n",
    "rgb_norm = np.clip(rgb / 3000, 0, 1)\n",
    "\n",
    "# Colormap for predictions\n",
    "cmap_green = plt.cm.RdYlGn.copy()\n",
    "cmap_green.set_bad(color='gray')\n",
    "\n",
    "# Difference colormap: gray, red, blue, green\n",
    "diff_cmap = ListedColormap(['#808080', '#FF6B6B', '#4DABF7', '#51CF66'])\n",
    "\n",
    "def create_diff_map(pred, gt):\n",
    "    pred = np.nan_to_num(pred, nan=0).astype(int)\n",
    "    gt = gt.astype(int)\n",
    "    diff = np.zeros_like(pred)\n",
    "    diff[(pred == 0) & (gt == 0)] = 0  # True Negative\n",
    "    diff[(pred == 1) & (gt == 0)] = 1  # False Positive\n",
    "    diff[(pred == 0) & (gt == 1)] = 2  # False Negative\n",
    "    diff[(pred == 1) & (gt == 1)] = 3  # True Positive\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main comparison grid - all predictions\n",
    "n_models = len(predictions)\n",
    "n_cols = min(n_models + 2, 4)  # +2 for RGB and WorldCover\n",
    "n_rows = int(np.ceil((n_models + 2) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "\n",
    "# RGB\n",
    "axes[0].imshow(rgb_norm)\n",
    "axes[0].set_title('Sentinel-2 RGB', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# WorldCover\n",
    "axes[1].imshow(worldcover_green, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[1].set_title(f'WorldCover\\n{wc_pct:.1f}%', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Each model prediction\n",
    "for i, (model_name, pred) in enumerate(predictions.items()):\n",
    "    ax = axes[i + 2]\n",
    "    pred_masked = np.ma.masked_invalid(pred)\n",
    "    ax.imshow(pred_masked, cmap=cmap_green, vmin=0, vmax=1)\n",
    "    ax.set_title(f'{model_name}\\n{green_percentages[model_name]:.1f}%', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(n_models + 2, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'{CITY} - All Model Predictions', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FOLDER, 'all_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference maps vs WorldCover\n",
    "n_models = len(predictions)\n",
    "fig, axes = plt.subplots(2, max(n_models, 3), figsize=(5*max(n_models, 3), 8))\n",
    "\n",
    "# Row 1: Predictions\n",
    "for i, (model_name, pred) in enumerate(predictions.items()):\n",
    "    pred_masked = np.ma.masked_invalid(pred)\n",
    "    axes[0, i].imshow(pred_masked, cmap=cmap_green, vmin=0, vmax=1)\n",
    "    axes[0, i].set_title(f'{model_name}\\n{green_percentages[model_name]:.1f}%', fontsize=12, fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Row 2: Difference maps\n",
    "for i, (model_name, pred) in enumerate(predictions.items()):\n",
    "    diff_map = create_diff_map(pred, worldcover_green)\n",
    "    axes[1, i].imshow(diff_map, cmap=diff_cmap, vmin=0, vmax=3)\n",
    "    axes[1, i].set_title(f'{model_name} vs WorldCover', fontsize=12, fontweight='bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Hide unused axes\n",
    "for i in range(n_models, max(n_models, 3)):\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#808080', label='Both: Non-Green (TN)'),\n",
    "    Patch(facecolor='#FF6B6B', label='Pred only: Green (FP)'),\n",
    "    Patch(facecolor='#4DABF7', label='GT only: Green (FN)'),\n",
    "    Patch(facecolor='#51CF66', label='Both: Green (TP)')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=4, fontsize=10, bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.suptitle(f'{CITY} - Model Predictions vs WorldCover', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FOLDER, 'difference_maps_worldcover.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics bar charts - vs WorldCover only\n",
    "df_wc = df_results[df_results['Method'].str.contains('WorldCover')].copy()\n",
    "df_wc['Model'] = df_wc['Method'].str.replace(' vs WorldCover', '')\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 5))\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'IoU']\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(df_wc)))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    bars = axes[i].bar(range(len(df_wc)), df_wc[metric], color=colors)\n",
    "    axes[i].set_xticks(range(len(df_wc)))\n",
    "    axes[i].set_xticklabels(df_wc['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axes[i].set_title(metric, fontsize=14, fontweight='bold')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].axhline(y=0.8, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, val in zip(bars, df_wc[metric]):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{val:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'{CITY} - Model Comparison vs WorldCover', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FOLDER, 'metrics_comparison_barchart.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Green coverage comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "methods = list(green_percentages.keys())\n",
    "percentages = list(green_percentages.values())\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))\n",
    "\n",
    "bars = ax.bar(methods, percentages, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for bar, pct in zip(bars, percentages):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{pct:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Green Coverage (%)', fontsize=14)\n",
    "ax.set_title(f'{CITY} - Green Coverage by Method', fontsize=16, fontweight='bold')\n",
    "ax.set_ylim(0, max(percentages) * 1.2)\n",
    "ax.axhline(y=wc_pct, color='gray', linestyle='--', alpha=0.7, label=f'WorldCover: {wc_pct:.1f}%')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FOLDER, 'green_coverage_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices - vs WorldCover\n",
    "wc_cms = {k: v for k, v in confusion_matrices.items() if 'WorldCover' in k}\n",
    "n_cms = len(wc_cms)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_cms, figsize=(5*n_cms, 4))\n",
    "if n_cms == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, cm) in zip(axes, wc_cms.items()):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Non-Green', 'Green'],\n",
    "                yticklabels=['Non-Green', 'Green'])\n",
    "    ax.set_title(name.replace(' vs WorldCover', ''), fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual (WorldCover)')\n",
    "\n",
    "plt.suptitle(f'{CITY} - Confusion Matrices vs WorldCover', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FOLDER, 'confusion_matrices_worldcover.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of all metrics\n",
    "df_wc_metrics = df_wc[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'IoU']].set_index('Model')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(df_wc_metrics, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            vmin=0.4, vmax=1.0, ax=ax, cbar_kws={'label': 'Score'})\n",
    "ax.set_title(f'{CITY} - Metrics Heatmap (vs WorldCover)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FOLDER, 'metrics_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(f\"MULTI-MODEL EVALUATION SUMMARY - {CITY}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nGreen Coverage:\")\n",
    "for method, pct in green_percentages.items():\n",
    "    diff = pct - wc_pct if method != 'WorldCover' else 0\n",
    "    print(f\"  {method:20s}: {pct:6.2f}%\" + (f\"  (diff: {diff:+.2f}%)\" if diff != 0 else \"\"))\n",
    "\n",
    "print(f\"\\nModel Performance vs WorldCover:\")\n",
    "print(f\"  {'Model':<15} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'IoU':>10}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for _, row in df_wc.iterrows():\n",
    "    print(f\"  {row['Model']:<15} {row['Accuracy']:>10.4f} {row['Precision']:>10.4f} {row['Recall']:>10.4f} {row['F1-Score']:>10.4f} {row['IoU']:>10.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_f1_model = df_wc.loc[df_wc['F1-Score'].idxmax(), 'Model']\n",
    "best_f1_score = df_wc['F1-Score'].max()\n",
    "print(f\"\\nBest Model (F1-Score): {best_f1_model} ({best_f1_score:.4f})\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  {OUTPUT_FOLDER}/\")\n",
    "for f in os.listdir(OUTPUT_FOLDER):\n",
    "    print(f\"    - {f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON\n",
    "summary = {\n",
    "    \"city\": CITY,\n",
    "    \"models_evaluated\": list(predictions.keys()),\n",
    "    \"green_percentages\": green_percentages,\n",
    "    \"metrics_vs_worldcover\": df_wc.to_dict(orient='records'),\n",
    "    \"best_model\": {\n",
    "        \"name\": best_f1_model,\n",
    "        \"f1_score\": float(best_f1_score)\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"rf_threshold\": RF_THRESHOLD,\n",
    "        \"svm_threshold\": SVM_THRESHOLD,\n",
    "        \"unet_threshold\": UNET_THRESHOLD,\n",
    "        \"ndvi_threshold\": NDVI_THRESHOLD,\n",
    "        \"smoothing\": APPLY_SMOOTHING\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_FOLDER, 'evaluation_summary.json'), 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary saved to: {OUTPUT_FOLDER}/evaluation_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}