{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-City Green Space Detection\n",
    "## Training Support Vector Machine (SVM) with WorldCover 2021 as Ground Truth\n",
    "\n",
    "**Training Cities:** 9 cities for robust model training\n",
    "\n",
    "**Key Features:**\n",
    "- Uses **WorldCover 2021** as ground truth for training\n",
    "- Green classes: Tree cover (10), Shrubland (20), Grassland (30), Mangroves (95)\n",
    "- Multi-temporal Sentinel-2 data (April, August, November)\n",
    "- 21 bands: 4 spectral bands x 3 months + 3 vegetation indices x 3 months\n",
    "- **Cross-city training** for better generalization\n",
    "\n",
    "**SVM Implementation:**\n",
    "- Uses SGDClassifier with hinge loss (linear SVM approximation) for scalability\n",
    "- Optional RBF kernel SVM with subsampling for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Base paths - using relative paths from project root\n# Run notebooks from the project root directory: python -m jupyter notebook\nimport os\n\n# Find project root (go up from notebooks/training/)\nif os.path.exists(\"data\") and os.path.exists(\"models\"):\n    PROJECT_ROOT = os.getcwd()\nelif os.path.exists(\"../../data\") and os.path.exists(\"../../models\"):\n    PROJECT_ROOT = os.path.abspath(\"../..\")\nelse:\n    PROJECT_ROOT = os.getcwd()\n    print(f\"Warning: Could not detect project root. Using: {PROJECT_ROOT}\")\n\n# Derived paths\nDATA_PATH = os.path.join(PROJECT_ROOT, \"data\")\nMODELS_PATH = os.path.join(PROJECT_ROOT, \"models\")\nGEOJSON_FOLDER = os.path.join(DATA_PATH, \"aois\")\n\n# Output folder\nOUTPUT_FOLDER = os.path.join(PROJECT_ROOT, \"outputs\", \"svm_training\")\nos.makedirs(OUTPUT_FOLDER, exist_ok=True)\nos.makedirs(MODELS_PATH, exist_ok=True)\n\n# Create timestamped run folder\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nRUN_FOLDER = os.path.join(OUTPUT_FOLDER, f\"run_{timestamp}\")\nos.makedirs(RUN_FOLDER, exist_ok=True)\n\n# WorldCover green classes\nGREEN_CLASSES = [10, 20, 30, 95]  # Tree, Shrub, Grass, Mangroves\n\n# SVM Configuration\nUSE_RBF_KERNEL = False  # Set to True for RBF kernel (slower but potentially better)\nMAX_SAMPLES_FOR_RBF = 50000  # RBF SVM is slow, limit samples\n\n# Define cities with their specific file locations\nCITY_FILES = {\n    \"Amsterdam\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Amsterdam_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Amsterdam_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Amsterdam.geojson\"),\n    },\n    \"Auckland\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Auckland_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Auckland_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Auckland.geojson\"),\n    },\n    \"Barcelona\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Barcelona_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Barcelona_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Barcelona.geojson\"),\n    },\n    \"Sydney\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Sydney_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Sydney_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Sydney.geojson\"),\n    },\n    \"Toronto\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Toronto_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Toronto_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Toronto.geojson\"),\n    },\n    \"Vienna\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Wien_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Vienna_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Vienna.geojson\"),\n    },\n    \"London\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"London_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"London_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"London.geojson\"),\n    },\n    \"Melbourne\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Melbourne_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Melbourne_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Melbourne.geojson\"),\n    },\n    \"Paris\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Paris_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Paris_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Paris.geojson\"),\n    },\n    \"San_Francisco\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"San_Francisco_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"San_Francisco_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"San_Francisco.geojson\"),\n    },\n    \"Seattle\": {\n        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Seattle_MultiMonth_stack.tif\"),\n        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Seattle_WorldCover_2021.tif\"),\n        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Seattle.geojson\"),\n    },\n}\n\nprint(\"Configuration loaded\")\nprint(f\"  Project root: {PROJECT_ROOT}\")\nprint(f\"  Data path: {DATA_PATH}\")\nprint(f\"  Models path: {MODELS_PATH}\")\nprint(f\"  Output folder: {RUN_FOLDER}\")\nprint(f\"  SVM Type: {'RBF Kernel' if USE_RBF_KERNEL else 'Linear (SGD)'}\")\nprint(f\"  Target cities: {len(CITY_FILES)}\")\nfor city in CITY_FILES:\n    print(f\"    - {city}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Discover Available Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DISCOVERING AVAILABLE CITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nChecking {len(CITY_FILES)} configured cities...\")\n",
    "\n",
    "cities_data = []\n",
    "missing_cities = []\n",
    "\n",
    "for city_name, paths in CITY_FILES.items():\n",
    "    stack_file = paths[\"stack\"]\n",
    "    worldcover_file = paths[\"worldcover\"]\n",
    "    geojson_file = paths[\"geojson\"]\n",
    "    \n",
    "    has_stack = os.path.exists(stack_file)\n",
    "    has_worldcover = os.path.exists(worldcover_file)\n",
    "    has_geojson = os.path.exists(geojson_file)\n",
    "    \n",
    "    status_stack = \"Y\" if has_stack else \"N\"\n",
    "    status_geojson = \"Y\" if has_geojson else \"N\"\n",
    "    status_worldcover = \"Y\" if has_worldcover else \"N\"\n",
    "    \n",
    "    print(f\"  {city_name:15s} - Stack: {status_stack}  GeoJSON: {status_geojson}  WorldCover: {status_worldcover}\")\n",
    "    \n",
    "    if has_stack and has_geojson and has_worldcover:\n",
    "        cities_data.append({\n",
    "            \"name\": city_name,\n",
    "            \"stack_file\": stack_file,\n",
    "            \"geojson_file\": geojson_file,\n",
    "            \"worldcover_file\": worldcover_file\n",
    "        })\n",
    "    else:\n",
    "        missing = []\n",
    "        if not has_stack: missing.append(\"Stack\")\n",
    "        if not has_geojson: missing.append(\"GeoJSON\")\n",
    "        if not has_worldcover: missing.append(\"WorldCover\")\n",
    "        missing_cities.append(f\"{city_name} (missing: {', '.join(missing)})\")\n",
    "\n",
    "complete_cities = cities_data\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Cities with complete data: {len(complete_cities)}/{len(CITY_FILES)}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if len(complete_cities) == 0:\n",
    "    raise ValueError(\"No cities with complete data found!\")\n",
    "\n",
    "print(f\"\\nReady to train with {len(complete_cities)} cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Load and Process All Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING AND PROCESSING ALL CITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "EXPECTED_BANDS = None\n",
    "MAX_SAMPLES_PER_CLASS_PER_CITY = 50000  # Reduced for SVM (memory constraints)\n",
    "\n",
    "all_X = []\n",
    "all_y = []\n",
    "city_info = []\n",
    "skipped_cities = []\n",
    "\n",
    "for city_data in tqdm(complete_cities, desc=\"Processing cities\"):\n",
    "    city_name = city_data[\"name\"]\n",
    "    stack_file = city_data[\"stack_file\"]\n",
    "    worldcover_file = city_data[\"worldcover_file\"]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing: {city_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Load Sentinel-2 stack\n",
    "        with rasterio.open(stack_file) as src:\n",
    "            X_stack = src.read()\n",
    "            stack_transform = src.transform\n",
    "            stack_shape = (src.height, src.width)\n",
    "            stack_crs = src.crs\n",
    "        \n",
    "        n_bands = X_stack.shape[0]\n",
    "        print(f\"  Loaded Sentinel-2 stack: {X_stack.shape} ({n_bands} bands)\")\n",
    "        \n",
    "        if EXPECTED_BANDS is None:\n",
    "            EXPECTED_BANDS = n_bands\n",
    "            print(f\"  Setting expected bands to {EXPECTED_BANDS}\")\n",
    "        elif n_bands != EXPECTED_BANDS:\n",
    "            print(f\"  SKIPPING: Expected {EXPECTED_BANDS} bands, but found {n_bands}\")\n",
    "            skipped_cities.append({\"name\": city_name, \"reason\": f\"Band mismatch\"})\n",
    "            continue\n",
    "        \n",
    "        # Load and reproject WorldCover\n",
    "        with rasterio.open(worldcover_file) as src:\n",
    "            worldcover_data = np.empty(stack_shape, dtype=np.uint8)\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=worldcover_data,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=stack_transform,\n",
    "                dst_crs=stack_crs,\n",
    "                resampling=Resampling.nearest\n",
    "            )\n",
    "        \n",
    "        labels = np.isin(worldcover_data, GREEN_CLASSES).astype(np.uint8)\n",
    "        green_percentage = 100 * labels.sum() / labels.size\n",
    "        print(f\"  WorldCover labels: {labels.shape} ({green_percentage:.2f}% green)\")\n",
    "        \n",
    "        # Reshape for sklearn\n",
    "        n_pixels = X_stack.shape[1] * X_stack.shape[2]\n",
    "        X = X_stack.reshape(n_bands, -1).T\n",
    "        y = labels.flatten()\n",
    "        \n",
    "        # Remove NaN values\n",
    "        valid_mask = ~np.isnan(X).any(axis=1)\n",
    "        X_clean = X[valid_mask]\n",
    "        y_clean = y[valid_mask]\n",
    "        \n",
    "        print(f\"  Valid samples: {len(X_clean):,}\")\n",
    "        \n",
    "        # Balanced sampling\n",
    "        green_indices = np.where(y_clean == 1)[0]\n",
    "        nongreen_indices = np.where(y_clean == 0)[0]\n",
    "        \n",
    "        n_samples_per_class = min(len(green_indices), len(nongreen_indices), MAX_SAMPLES_PER_CLASS_PER_CITY)\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        sampled_green = np.random.choice(green_indices, n_samples_per_class, replace=False)\n",
    "        sampled_nongreen = np.random.choice(nongreen_indices, n_samples_per_class, replace=False)\n",
    "        \n",
    "        sampled_indices = np.concatenate([sampled_green, sampled_nongreen])\n",
    "        np.random.shuffle(sampled_indices)\n",
    "        \n",
    "        X_sampled = X_clean[sampled_indices]\n",
    "        y_sampled = y_clean[sampled_indices]\n",
    "        \n",
    "        print(f\"  Balanced sampling: {len(X_sampled):,} samples ({n_samples_per_class:,} per class)\")\n",
    "        \n",
    "        all_X.append(X_sampled)\n",
    "        all_y.append(y_sampled)\n",
    "        city_info.extend([city_name] * len(X_sampled))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {city_name}: {e}\")\n",
    "        skipped_cities.append({\"name\": city_name, \"reason\": str(e)})\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATA AGGREGATION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if len(all_X) == 0:\n",
    "    raise ValueError(\"No valid city data loaded!\")\n",
    "\n",
    "X_combined = np.vstack(all_X)\n",
    "y_combined = np.hstack(all_y)\n",
    "city_info = np.array(city_info)\n",
    "\n",
    "print(f\"\\nCombined dataset:\")\n",
    "print(f\"  Cities included: {len(all_X)}\")\n",
    "print(f\"  Total samples: {len(X_combined):,}\")\n",
    "print(f\"  Features (bands): {X_combined.shape[1]}\")\n",
    "print(f\"  Green samples: {np.sum(y_combined == 1):,} ({100*np.sum(y_combined == 1)/len(y_combined):.1f}%)\")\n",
    "print(f\"  Non-green samples: {np.sum(y_combined == 0):,} ({100*np.sum(y_combined == 0)/len(y_combined):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split & Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN-TEST SPLIT & FEATURE NORMALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split data (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y_combined, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_combined\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"  Testing samples: {len(X_test):,}\")\n",
    "\n",
    "# Feature normalization (CRITICAL for SVM!)\n",
    "print(f\"\\nApplying feature normalization (StandardScaler)...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"  Features normalized\")\n",
    "print(f\"    Mean (train): {X_train_scaled.mean():.4f}\")\n",
    "print(f\"    Std (train): {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Train SVM Model\n",
    "\n",
    "Two options:\n",
    "1. **Linear SVM (SGDClassifier)**: Fast, scales to large datasets\n",
    "2. **RBF Kernel SVM**: Better for non-linear boundaries, but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SVM MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if USE_RBF_KERNEL:\n",
    "    # RBF Kernel SVM (slower but potentially better)\n",
    "    print(f\"\\nUsing RBF Kernel SVM\")\n",
    "    \n",
    "    # Subsample for RBF SVM if dataset is too large\n",
    "    if len(X_train_scaled) > MAX_SAMPLES_FOR_RBF:\n",
    "        print(f\"  Subsampling to {MAX_SAMPLES_FOR_RBF:,} samples for RBF kernel...\")\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.choice(len(X_train_scaled), MAX_SAMPLES_FOR_RBF, replace=False)\n",
    "        X_train_svm = X_train_scaled[indices]\n",
    "        y_train_svm = y_train[indices]\n",
    "    else:\n",
    "        X_train_svm = X_train_scaled\n",
    "        y_train_svm = y_train\n",
    "    \n",
    "    svm = SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        verbose=True,\n",
    "        probability=True  # Enable probability estimates\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSVM Parameters:\")\n",
    "    print(f\"  kernel: rbf\")\n",
    "    print(f\"  C: 1.0\")\n",
    "    print(f\"  gamma: scale\")\n",
    "    print(f\"  Training samples: {len(X_train_svm):,}\")\n",
    "    \n",
    "    print(f\"\\nTraining RBF SVM (this may take a while)...\")\n",
    "    svm.fit(X_train_svm, y_train_svm)\n",
    "    \n",
    "else:\n",
    "    # Linear SVM using SGDClassifier (fast and scalable)\n",
    "    print(f\"\\nUsing Linear SVM (SGDClassifier with hinge loss)\")\n",
    "    \n",
    "    svm = SGDClassifier(\n",
    "        loss='hinge',  # Hinge loss = linear SVM\n",
    "        penalty='l2',\n",
    "        alpha=0.0001,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSGDClassifier Parameters:\")\n",
    "    print(f\"  loss: hinge (linear SVM)\")\n",
    "    print(f\"  penalty: l2\")\n",
    "    print(f\"  alpha: 0.0001\")\n",
    "    print(f\"  max_iter: 1000\")\n",
    "    print(f\"  class_weight: balanced\")\n",
    "    print(f\"  Training samples: {len(X_train_scaled):,}\")\n",
    "    \n",
    "    print(f\"\\nTraining Linear SVM...\")\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nModel trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"\\nModel Performance (SVM):\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"               Non-Green  Green\")\n",
    "print(f\"Actual Non-Green  {cm[0,0]:>8,}  {cm[0,1]:>8,}\")\n",
    "print(f\"       Green      {cm[1,0]:>8,}  {cm[1,1]:>8,}\")\n",
    "\n",
    "# Green detection analysis\n",
    "total_actual_green = cm[1,0] + cm[1,1]\n",
    "green_detected = 100 * cm[1,1] / total_actual_green if total_actual_green > 0 else 0\n",
    "green_missed = 100 * cm[1,0] / total_actual_green if total_actual_green > 0 else 0\n",
    "\n",
    "print(f\"\\nGreen Detection Analysis:\")\n",
    "print(f\"  Green correctly detected: {green_detected:.1f}%\")\n",
    "print(f\"  Green missed: {green_missed:.1f}%\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    \"model\": \"SVM_RBF\" if USE_RBF_KERNEL else \"SVM_Linear\",\n",
    "    \"ground_truth\": \"WorldCover_2021\",\n",
    "    \"training_cities\": [city['name'] for city in complete_cities],\n",
    "    \"n_cities\": len(complete_cities),\n",
    "    \"total_training_samples\": int(len(X_train)),\n",
    "    \"total_testing_samples\": int(len(X_test)),\n",
    "    \"accuracy\": float(accuracy),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"confusion_matrix\": cm.tolist()\n",
    "}\n",
    "\n",
    "with open(os.path.join(RUN_FOLDER, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetrics saved to: {RUN_FOLDER}/metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Green', 'Green'],\n",
    "            yticklabels=['Non-Green', 'Green'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - SVM ({\"RBF\" if USE_RBF_KERNEL else \"Linear\"})\\n(Trained on {len(complete_cities)} cities)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RUN_FOLDER, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis (Linear SVM Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_RBF_KERNEL:\n",
    "    # For linear SVM, we can look at coefficient magnitudes as feature importance\n",
    "    importances = np.abs(svm.coef_[0])\n",
    "    n_features = len(importances)\n",
    "    \n",
    "    # Generate band names\n",
    "    if n_features == 21:\n",
    "        band_names = [\n",
    "            'B02-Apr', 'B03-Apr', 'B04-Apr', 'B08-Apr', 'NDVI-Apr', 'EVI-Apr', 'SAVI-Apr',\n",
    "            'B02-Aug', 'B03-Aug', 'B04-Aug', 'B08-Aug', 'NDVI-Aug', 'EVI-Aug', 'SAVI-Aug',\n",
    "            'B02-Nov', 'B03-Nov', 'B04-Nov', 'B08-Nov', 'NDVI-Nov', 'EVI-Nov', 'SAVI-Nov'\n",
    "        ]\n",
    "    else:\n",
    "        band_names = [f'Band_{i+1}' for i in range(n_features)]\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, max(8, n_features * 0.4)))\n",
    "    plt.barh(range(len(importances)), importances[indices], color='steelblue')\n",
    "    plt.yticks(range(len(importances)), [band_names[i] for i in indices])\n",
    "    plt.xlabel('|Coefficient| (Feature Importance)', fontsize=12)\n",
    "    plt.title(f'Linear SVM Feature Importance\\n(Trained on {len(all_X)} cities, {n_features} bands)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RUN_FOLDER, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature importance plot saved\")\n",
    "    print(f\"\\nTop 10 most important features:\")\n",
    "    for i in range(min(10, len(importances))):\n",
    "        idx = indices[i]\n",
    "        print(f\"  {i+1:2d}. {band_names[idx]:12s}: {importances[idx]:.4f}\")\n",
    "else:\n",
    "    print(\"Feature importance not available for RBF kernel SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "model_file = os.path.join(RUN_FOLDER, 'svm_model.pkl')\n",
    "joblib.dump(svm, model_file)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_file = os.path.join(RUN_FOLDER, 'feature_scaler.pkl')\n",
    "joblib.dump(scaler, scaler_file)\n",
    "\n",
    "print(f\"Model saved to: {model_file}\")\n",
    "print(f\"Scaler saved to: {scaler_file}\")\n",
    "print(f\"\\nTo load the model and scaler later:\")\n",
    "print(f\"  import joblib\")\n",
    "print(f\"  svm = joblib.load('{model_file}')\")\n",
    "print(f\"  scaler = joblib.load('{scaler_file}')\")\n",
    "print(f\"\\nIMPORTANT: Always apply scaler.transform(X) before prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 11. Per-City Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CITY PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "per_city_results = []\n",
    "\n",
    "for city_data in complete_cities:\n",
    "    city_name = city_data[\"name\"]\n",
    "    stack_file = city_data[\"stack_file\"]\n",
    "    worldcover_file = city_data[\"worldcover_file\"]\n",
    "    \n",
    "    print(f\"\\n{city_name}:\")\n",
    "    \n",
    "    try:\n",
    "        # Load city data\n",
    "        with rasterio.open(stack_file) as src:\n",
    "            X_stack = src.read()\n",
    "            stack_transform = src.transform\n",
    "            stack_shape = (src.height, src.width)\n",
    "            stack_crs = src.crs\n",
    "        \n",
    "        with rasterio.open(worldcover_file) as src:\n",
    "            worldcover_data = np.empty(stack_shape, dtype=np.uint8)\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=worldcover_data,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=stack_transform,\n",
    "                dst_crs=stack_crs,\n",
    "                resampling=Resampling.nearest\n",
    "            )\n",
    "        \n",
    "        labels = np.isin(worldcover_data, GREEN_CLASSES).astype(np.uint8)\n",
    "        \n",
    "        # Reshape and clean\n",
    "        X = X_stack.reshape(X_stack.shape[0], -1).T\n",
    "        y = labels.flatten()\n",
    "        valid_mask = ~np.isnan(X).any(axis=1)\n",
    "        X_city = X[valid_mask]\n",
    "        y_city = y[valid_mask]\n",
    "        \n",
    "        # Apply scaler\n",
    "        X_city_scaled = scaler.transform(X_city)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_city = svm.predict(X_city_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_city, y_pred_city)\n",
    "        prec = precision_score(y_city, y_pred_city, zero_division=0)\n",
    "        rec = recall_score(y_city, y_pred_city, zero_division=0)\n",
    "        f1_city = f1_score(y_city, y_pred_city, zero_division=0)\n",
    "        \n",
    "        # Calculate green percentages\n",
    "        gt_green_pct = 100 * np.sum(y_city == 1) / len(y_city)\n",
    "        pred_green_pct = 100 * np.sum(y_pred_city == 1) / len(y_pred_city)\n",
    "        diff_pct = pred_green_pct - gt_green_pct\n",
    "        \n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {prec:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1-Score:  {f1_city:.4f}\")\n",
    "        print(f\"  Green %:   GT={gt_green_pct:.1f}%  Pred={pred_green_pct:.1f}%  (diff: {diff_pct:+.1f}%)\")\n",
    "        \n",
    "        per_city_results.append({\n",
    "            \"city\": city_name,\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "            \"f1_score\": float(f1_city),\n",
    "            \"gt_green_pct\": float(gt_green_pct),\n",
    "            \"pred_green_pct\": float(pred_green_pct),\n",
    "            \"diff_pct\": float(diff_pct)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "# Save per-city results\n",
    "with open(os.path.join(RUN_FOLDER, \"per_city_metrics.json\"), \"w\") as f:\n",
    "    json.dump(per_city_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Per-city metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SVM TRAINING - SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nGround Truth: WorldCover 2021\")\n",
    "print(f\"Green Classes: Tree cover (10), Shrubland (20), Grassland (30), Mangroves (95)\")\n",
    "\n",
    "print(f\"\\nSVM Configuration:\")\n",
    "if USE_RBF_KERNEL:\n",
    "    print(f\"  Type: RBF Kernel SVM\")\n",
    "    print(f\"  C: 1.0\")\n",
    "    print(f\"  gamma: scale\")\n",
    "else:\n",
    "    print(f\"  Type: Linear SVM (SGDClassifier)\")\n",
    "    print(f\"  Loss: hinge\")\n",
    "    print(f\"  Alpha: 0.0001\")\n",
    "\n",
    "print(f\"\\nTraining Data:\")\n",
    "print(f\"  Cities: {len(complete_cities)}\")\n",
    "for city in complete_cities:\n",
    "    print(f\"    - {city['name']}\")\n",
    "\n",
    "print(f\"\\n  Total training samples: {len(X_train):,}\")\n",
    "print(f\"  Total testing samples:  {len(X_test):,}\")\n",
    "\n",
    "print(f\"\\nModel Performance (Overall):\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "if per_city_results:\n",
    "    print(f\"\\nPer-City Performance (Average):\")\n",
    "    avg_acc = np.mean([r['accuracy'] for r in per_city_results])\n",
    "    avg_prec = np.mean([r['precision'] for r in per_city_results])\n",
    "    avg_rec = np.mean([r['recall'] for r in per_city_results])\n",
    "    avg_f1 = np.mean([r['f1_score'] for r in per_city_results])\n",
    "    avg_diff = np.mean([r['diff_pct'] for r in per_city_results])\n",
    "    print(f\"  Accuracy:  {avg_acc:.4f}\")\n",
    "    print(f\"  Precision: {avg_prec:.4f}\")\n",
    "    print(f\"  Recall:    {avg_rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {avg_f1:.4f}\")\n",
    "    print(f\"  Avg Green % Difference: {avg_diff:+.2f}%\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Results folder: {RUN_FOLDER}\")\n",
    "print(f\"  - metrics.json\")\n",
    "print(f\"  - per_city_metrics.json\")\n",
    "print(f\"  - confusion_matrix.png\")\n",
    "if not USE_RBF_KERNEL:\n",
    "    print(f\"  - feature_importance.png\")\n",
    "print(f\"  - svm_model.pkl\")\n",
    "print(f\"  - feature_scaler.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"TRAINING COMPLETE!\")\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 13. Copy Model to Project Root (Optional)\n",
    "\n",
    "Run this cell to copy the trained model to the project root for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "import shutil\n\n# Copy model to main models folder\nsrc_model = os.path.join(RUN_FOLDER, 'svm_model.pkl')\nsrc_scaler = os.path.join(RUN_FOLDER, 'feature_scaler.pkl')\n\ndst_model = os.path.join(MODELS_PATH, 'svm_model.pkl')\ndst_scaler = os.path.join(MODELS_PATH, 'svm_scaler.pkl')\n\nshutil.copy(src_model, dst_model)\nshutil.copy(src_scaler, dst_scaler)\n\nprint(f\"Model copied to: {dst_model}\")\nprint(f\"Scaler copied to: {dst_scaler}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}